# Logic-of-Thought
Code for ``Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models``

## Setup
* python==3.10


## Experiments
### Datasets
In the folder ``datasets``, we have placed all the datasets required for the experiment.

In each dataset folder, the ``json`` folder places the dataset and the ``npy`` folder places the true answer in .npy file format.

1. In FOLIO folder, you should use ``val.json`` and ``val.npy`` for ``main results``.

2. In logiqa folder, you should use ``test.json`` and ``test.npy`` for ``main results``.

3. It should be noted that ``dev100.json`` contains 100 pieces of data for ``ToT``, and ``dev100logic.json`` contains 100 pieces of data for ``LoT+ToT``. Alternatively, you can conduct an experiment using LoT on dev100.json and use the extended context generated by lot as input to ToT.

4. In the ReClor folder, you should use  ``arlsat_test.json`` and ``arlsat_test.npy`` for ``main results`` and ``SatLM``

5. In ruletaker folder, you should use ``test.json`` and ``test.npy`` for ``main results``






### Main results
#### 1. Testing with GPT-3.5-turbo/GPT-4 on the ReClor and LogiQA dataset
Using files in path `experiments\lot_4`

#### 2. Testing with GPT-3.5-turbo-instruct on the ReClor dataset
Using files in path `experiments\lot_instruct`

#### 3. Testing with GPT-3.5-turbo/GPT-4 on the RuleTaker, ProofWriter and FOLIO dataset
Using files in path `experiments\lot_2`


```
# Experimental steps:

1. pip install -r requirements.txt

2. Fill in the dataset path and key and choose the type of LLMs at the beginning of the file: direct.py, cot.py, manual.py

3. Then you can run direct.py, cot.py, lot.py, lotcot.py to test the direct/cot/lot/lotcot method.

4. Generate Self-Consistency method's results based on the experimental results using experiments\npy.ipynb

5. You can fill the generated .npy file into experiments\npy.ipynb to test accuracy.
```



### SatLM
We borrowed the SatLM code from [SatLM](https://github.com/xiye17/SAT-LM), and made some modifications according to the characteristics of the dataset.

#### Testing with GPT-3.5-turbo-instruct on the ReClor dataset
Using files in path `experiments\SAT-LM`

```
# Experimental steps:
1. pip install -r requirements.txt

2. SatLM: You can refer to the instructions of SAT-LM or run:
python SAT-LM/run_multistage.py --task arlsat --run_pred --batch_size 5 --num_samples 1 --temperature 0.0 --sig_prompt_id sigz3 --trans_setting setupsatlm --eval_split test --num_dev -1

3. LoT: Refer to the "Testing with GPT-3.5-turbo-instruct on the ReClor and LogiQA dataset"
```


### ToT
We borrowed the ToT code from [cumulative-reasoning](https://github.com/iiis-ai/cumulative-reasoning) and [DetermLR](https://github.com/XiaoMi/DetermLR), and made some modifications according to the characteristics of the dataset.

#### Testing with GPT-4 on the ProofWriter dataset
Using files in path `experiments\ToT`

```
# Experimental steps:
1. pip install -r requirements.txt

2. Fill in the key in experiments\ToT\proofwriter-tot.py

3. Test ToT: propnum=4, reasoningnum=4, dataset=dataset=dataset=datasets\pw\json\dev100.json

4. Test direct: propnum=0, reasoningnum=0, dataset=dataset=datasets\pw\json\dev100.json

5. Test loT+ToT: propnum=4, reasoningnum=4, dataset=datasets\pw\json\dev100logic.json
```



